{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8aaafb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d6b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_00=\"/home/mustapha/Downloads/Predictive-Modeling-of-Ionic-Conductivity-in-Garnet-Type-Solid-Electrolytes-main/Data/data_65.csv\"\n",
    "file_path = \"/home/mustapha/Downloads/data_augmented.xlsx\"\n",
    "file_path1 = \"/home/mustapha/Downloads/Predictive-Modeling-of-Ionic-Conductivity-in-Garnet-Type-Solid-Electrolytes-main/Data/data_augmented.xlsx\"\n",
    "\n",
    "\n",
    "\n",
    "#df0 = pd.read_excel(file_path1)\n",
    "df = pd.read_excel(file_path)\n",
    "df1 = df.iloc[:176]\n",
    "df2 = df.iloc[176:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_material_formula(row):\n",
    "    # Extract the values from each column\n",
    "    a = row['Li stoichiometry']\n",
    "    b = row['La stoichiometry']\n",
    "    c = row['Zr stoichiometry']\n",
    "    x = row['Li site dopant stoichiometry']\n",
    "    y = row['La site dopant stoichiometry']\n",
    "    z = row['Zr site dopant stoichiometry']\n",
    "    M = row['li_dopant']\n",
    "    N = row['la_dopant']\n",
    "    K = row['zr_dopant']\n",
    "\n",
    "\n",
    "\n",
    "    formula = ''\n",
    "\n",
    "\n",
    "    if a > 0:\n",
    "        formula += f'Li{a:.2f}'\n",
    "\n",
    "    # Add M if x > 0\n",
    "    if x > 0:\n",
    "        formula += f'{M}{x:.2f}'\n",
    "\n",
    "    # Add La with its stoichiometry\n",
    "    if b > 0:\n",
    "        formula += f'La{b:.2f}'\n",
    "\n",
    "    # Add N if y > 0\n",
    "    if y > 0:\n",
    "        formula += f'{N}{y:.2f}'\n",
    "\n",
    "    # Add Zr with its stoichiometry\n",
    "    if c > 0:\n",
    "        formula += f'Zr{c:.2f}'\n",
    "\n",
    "    # Add K if z > 0\n",
    "    if z > 0:\n",
    "        formula += f'{K}{z:.2f}'\n",
    "        \n",
    "    formula += f'O12'\n",
    "   \n",
    "    return formula\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have a DataFrame named 'df' and want to create a new column 'MaterialFormula'\n",
    "df['MaterialFormula'] = df.apply(transform_to_material_formula, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f3d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def interpolate_relative_density(df):\n",
    "#    # Sort by 'MaterialFormula' and 'log_cond' to ensure correct interpolation\n",
    "#    df = df.sort_values(by=['MaterialFormula', 'Ionic conductivity']).reset_index(drop=True)\n",
    "\n",
    "#    # Interpolate 'Relative density' within each 'MaterialFormula' group\n",
    "#    interpolated = df.groupby('MaterialFormula')['Relative density'].apply(lambda group: group.interpolate(method='linear'))\n",
    "\n",
    "#    # Ensure the index matches the original DataFrame's index\n",
    "#    df['Relative density'] = interpolated.reset_index(level=0, drop=True)\n",
    "\n",
    "#   # Fill remaining missing values with forward fill and backward fill\n",
    "#    df['Relative density'] = df['Relative density'].ffill().bfill()\n",
    "\n",
    "#   return df\n",
    "\n",
    "\n",
    "\n",
    "#df = interpolate_relative_density(df)\n",
    "#df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c335482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "def knn_impute_relative_density(df, n_neighbors=5):\n",
    "    # Check if 'log_cond' has any missing values\n",
    "    if df['log_cond'].isna().all():\n",
    "        # If 'log_cond' is entirely NaN, skip KNN and just ffill/bfill\n",
    "        df['Relative density'] = df['Relative density'].ffill().bfill()\n",
    "    else:\n",
    "        # Select only the relevant columns for KNN imputation\n",
    "        impute_df = df[['log_cond', 'Relative density']]\n",
    "\n",
    "        # Initialize the KNNImputer\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "\n",
    "        # Apply KNN Imputer\n",
    "        imputed_values = imputer.fit_transform(impute_df)\n",
    "\n",
    "        # Replace 'Relative density' column with imputed values\n",
    "        df['Relative density'] = imputed_values[:, -1]  # Use the last column, which is 'Relative density'\n",
    "\n",
    "        # Fill remaining missing values with forward fill and backward fill\n",
    "        df['Relative density'] = df['Relative density'].ffill().bfill()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "df = knn_impute_relative_density(df)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b02c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "materials_to_remove = ['Li6.40Al0.20La3.00Zr2.00O12', 'Li6.25Al0.25La3.00Zr2.00O12']\n",
    "#df = df[~df['MaterialFormula'].isin(materials_to_remove)]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da61c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def check_monotonicity_with_deviation_removal(df, max_allowed_deviations, deviation_threshold):\n",
    "    filtered_data = []\n",
    "    non_increasing_materials = []\n",
    "\n",
    "    grouped = df.groupby('MaterialFormula')\n",
    "\n",
    "    for material, group in grouped:\n",
    "        group_sorted = group.sort_values('Relative density')\n",
    "        densities = group_sorted['Relative density'].values\n",
    "        conductivities = group_sorted['log_cond'].values\n",
    "\n",
    "        # Check for NaN or infinite values and remove them\n",
    "        if np.any(np.isnan(conductivities)):\n",
    "            print(f\"Skipping material {material} due to NaN values.\")\n",
    "            continue\n",
    "\n",
    "        if np.any(np.isinf(densities)) or np.any(np.isinf(conductivities)):\n",
    "            print(f\"Skipping material {material} due to infinite values.\")\n",
    "            continue\n",
    "\n",
    "        # Handle materials with only one unique density value\n",
    "        if len(np.unique(densities)) < 2:\n",
    "            print(f\"Including material {material} with a single unique 'Relative density' value.\")\n",
    "            filtered_data.append(group_sorted)\n",
    "            continue\n",
    "\n",
    "        # Fit a linear model to capture the average increasing trend\n",
    "        slope, intercept, _, _, _ = linregress(densities, conductivities)\n",
    "\n",
    "        # Check if the slope is positive\n",
    "        if slope <= 0:\n",
    "            non_increasing_materials.append(material)\n",
    "            continue\n",
    "\n",
    "        expected_trend = intercept + slope * densities\n",
    "        deviations = conductivities - expected_trend\n",
    "        significant_deviations = np.abs(deviations) > deviation_threshold\n",
    "\n",
    "        # If the number of significant deviations is within the allowed limit, filter them out\n",
    "        if significant_deviations.sum() <= max_allowed_deviations:\n",
    "            group_filtered = group_sorted[~significant_deviations]\n",
    "        else:\n",
    "            # Otherwise, skip the entire material\n",
    "            continue\n",
    "\n",
    "        filtered_data.append(group_filtered)\n",
    "\n",
    "    # Combine filtered data into a single DataFrame\n",
    "    filtered_df = pd.concat(filtered_data)\n",
    "\n",
    "    # Plot non-increasing materials\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    for material in non_increasing_materials:\n",
    "        group = df[df['MaterialFormula'] == material]\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.lineplot(\n",
    "            x='Relative density',\n",
    "            y='log_cond',\n",
    "            data=group,\n",
    "            marker='o',\n",
    "            label=material,\n",
    "        )\n",
    "        plt.title(f'Non-Increasing Trend for {material}')\n",
    "        plt.xlabel('Relative density')\n",
    "        plt.ylabel('log_cond')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # Remove non-increasing materials from the dataset\n",
    "    filtered_df = filtered_df[~filtered_df['MaterialFormula'].isin(non_increasing_materials)]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc23315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_top_materials(df, num_materials):\n",
    "\n",
    "    # Counting the occurrences of each material formula\n",
    "    material_counts = df['MaterialFormula'].value_counts()\n",
    "\n",
    "    # Getting the top N most repeated materials\n",
    "    top_materials = material_counts.nlargest(num_materials)\n",
    "\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Looping through the top materials and plotting their data\n",
    "    for selected_material in top_materials.index:\n",
    "        selected_material_data = df[df['MaterialFormula'] == selected_material]\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.lineplot(\n",
    "            x='Relative density',\n",
    "            y='log_cond',\n",
    "            data=selected_material_data,\n",
    "            marker='o',\n",
    "            label=selected_material,\n",
    "        )\n",
    "        plt.title(f'Line Plot for {selected_material}')\n",
    "        plt.xlabel('Relative density')\n",
    "        plt.ylabel('log_cond')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_top_materials(df, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025a7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = check_monotonicity_with_deviation_removal(df, max_allowed_deviations=10, deviation_threshold=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7247b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_electric_neutrality(df, tolerance):\n",
    "    # Ensure DataFrame has necessary columns\n",
    "    required_columns = [\n",
    "        'Li stoichiometry', 'La stoichiometry', 'Zr stoichiometry',\n",
    "        'Li site dopant stoichiometry', 'Li site dopant ion charge',\n",
    "        'La site dopant stoichiometry', 'La site dopant ion charge',\n",
    "        'Zr site dopant stoichiometry', 'Zr site dopant ion charge'\n",
    "    ]\n",
    "    \n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Column '{col}' is missing from the DataFrame\")\n",
    "\n",
    "    # Calculate the total charge for each row\n",
    "    df['Total Charge'] = (\n",
    "        df['Li stoichiometry'] * 1 +  # Li has a charge of +1\n",
    "        df['La stoichiometry'] * 3 +  # La has a charge of +3\n",
    "        df['Zr stoichiometry'] * 4 +  # Zr has a charge of +4\n",
    "        df['Li site dopant stoichiometry'] * df['Li site dopant ion charge'] +  # Li site dopant charge\n",
    "        df['La site dopant stoichiometry'] * df['La site dopant ion charge'] +  # La site dopant charge\n",
    "        df['Zr site dopant stoichiometry'] * df['Zr site dopant ion charge']  # Zr site dopant charge\n",
    "    )\n",
    "    \n",
    "    # Subtract 24 from the total charge to check neutrality\n",
    "    df['Charge Difference'] = df['Total Charge'] - 24\n",
    "\n",
    "    # Identify rows that do not satisfy neutrality within the specified tolerance\n",
    "    non_neutrality_indexes = df[df['Charge Difference'].abs() > tolerance].index\n",
    "\n",
    "    df_filtered = df.drop(non_neutrality_indexes)\n",
    "\n",
    "    # Plotting the charge difference\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df.index, df['Charge Difference'], alpha=0.7, edgecolors='w', s=100)\n",
    "    plt.axhline(y=0, color='r', linestyle='--', label='Neutrality Line (Charge = 24)')\n",
    "    plt.axhline(y=tolerance, color='g', linestyle='--', label=f'Neutrality Tolerance ±{tolerance}')\n",
    "    plt.axhline(y=-tolerance, color='g', linestyle='--')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Charge Difference (Total Charge - 24)')\n",
    "    plt.title('Electric Neutrality of the Material')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return df_filtered, non_neutrality_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f3317",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_materials(df, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321f3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d830e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, non_neutrality_indexes = plot_electric_neutrality(df, 0.2)\n",
    "\n",
    "# Inspect the filtered DataFrame and the indices of non-neutral rows\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d217ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.copy()\n",
    "X = X.drop(['log_cond'],axis=1)\n",
    "X = X.drop(['Publication year','Quality of ionic conductivity','source','Doping strategy'], axis=1)\n",
    "X = X.drop(['MaterialFormula'],axis=1)\n",
    "X = X.drop(['li_dopant','la_dopant','zr_dopant'], axis=1)\n",
    "X = X.drop(['Total Charge','Charge Difference'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3585e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=3)\n",
    "# Fit and transform the data to impute the missing values\n",
    "X_knn = knn_imputer.fit_transform(X)\n",
    "\n",
    "# Convert the imputed NumPy array back to a DataFrame and specify column names\n",
    "X_knn = pd.DataFrame(data=X_knn, columns=X.columns)\n",
    "\n",
    "# Assuming you already have 'Ionic conductivity' in the DataFrame\n",
    "Y = X_knn['Ionic conductivity']\n",
    "\n",
    "# Drop the 'Ionic conductivity' column from X\n",
    "X_knn = X_knn.drop('Ionic conductivity', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd69826",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_knn.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d359201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import pandas as pd\n",
    "\n",
    "# Define a list of regression models to test\n",
    "models = [\n",
    "    HistGradientBoostingRegressor(),\n",
    "    LinearRegression(),\n",
    "    Ridge(),\n",
    "    RandomForestRegressor(),\n",
    "    GradientBoostingRegressor(),\n",
    "    KNeighborsRegressor(),\n",
    "    XGBRegressor(),\n",
    "    LGBMRegressor(),\n",
    "    CatBoostRegressor()\n",
    "]\n",
    "\n",
    "# Set the test split size\n",
    "test_size = 0.20\n",
    "\n",
    "# Preprocessing steps\n",
    "# Define numerical and categorical features\n",
    "numeric_features = X_knn.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X_knn.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing for numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over the models and evaluate each one\n",
    "for model in models:\n",
    "    # Splitting the data into training and testing sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_knn, Y, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Create a pipeline that includes preprocessing and the model\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('model', model)])\n",
    "\n",
    "    # Fit the model and make predictions\n",
    "    pipeline.fit(X_train, Y_train)\n",
    "    Y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(Y_test, Y_test_pred)\n",
    "    r2 = r2_score(Y_test, Y_test_pred)\n",
    "\n",
    "    # Record the results\n",
    "    results.append({\n",
    "        \"Model\": type(model).__name__,\n",
    "        \"R-squared (R2)\": r2,\n",
    "        \"Mean Squared Error (MSE)\": mse\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c39da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aff1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ddcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7884a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(by='R-squared (R2)', ascending=False, inplace=True)\n",
    "\n",
    "results_df.reset_index(drop=True, inplace=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff10384",
   "metadata": {},
   "source": [
    "## CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a0ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Define the groups\n",
    "GROUP1 = [\n",
    "    #['Li stoichiometry', 'La stoichiometry', 'Zr stoichiometry'],\n",
    "    #['Li site dopant stoichiometry', 'La site dopant stoichiometry', 'Zr site dopant stoichiometry']\n",
    "    ['Li stoichiometry', 'La stoichiometry', 'Zr stoichiometry','Relative density'],\n",
    "    ['Li site dopant stoichiometry', 'La site dopant stoichiometry', 'Zr site dopant stoichiometry','Relative density']\n",
    "]\n",
    "\n",
    "GROUP2 = [\n",
    "    ['Li site dopant ionic radius', 'La site dopant ionic radius', 'Zr site dopant ionic radius'],\n",
    "    ['Li site dopant atomic radius', 'La site dopant atomic radius', 'Zr site dopant atomic radius pm'],\n",
    "    ['Li site dopant electron affinity', 'La site dopant electron affinity', 'Zr site dopant electron affinity'],\n",
    "    ['Li site dopant e_ionisation', 'La site dopant e_ionisation', 'Zr site dopant e_ionisation'],\n",
    "    ['Li site dopant atomic number', 'La site dopant atomic number', 'Zr site dopant atomic number'],\n",
    "    ['Li site dopant molar mass', 'La site dopant molar mass', 'Zr site dopant molar mass'],\n",
    "    ['Li site dopant electroneg.', 'La site dopant electroneg.', 'Zr site dopant electroneg.'],\n",
    "    ['Li site dopant ion charge', 'La site dopant ion charge', 'Zr site dopant ion charge'],\n",
    "    ['Li site dopant crystal rad.', 'La site dopant crystal rad.', 'Zr site dopant crystal rad.']\n",
    "]\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over all combinations of one or more lines from GROUP1\n",
    "for group1_combination in itertools.combinations(GROUP1, r=1):\n",
    "    \n",
    "    # Iterate over all combinations of one or more lines from GROUP2\n",
    "    for r in range(1, len(GROUP2) + 1):\n",
    "        for group2_combination in itertools.combinations(GROUP2, r=r):\n",
    "            \n",
    "            # Combine the selected features from both groups\n",
    "            selected_features = [cols for sublist in group1_combination for cols in sublist] + \\\n",
    "                                [cols for sublist in group2_combination for cols in sublist]\n",
    "            \n",
    "            # Select only the features for this combination\n",
    "            X_temp = X_knn[selected_features]\n",
    "            \n",
    "            # Split the data\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(X_temp, Y, test_size=0.2, random_state=42)\n",
    "            \n",
    "            # Train the model using CatBoostRegressor\n",
    "            model = CatBoostRegressor()\n",
    "            model.fit(X_train, Y_train)\n",
    "            Y_test_pred = model.predict(X_test)\n",
    "            \n",
    "            # Evaluate the model\n",
    "            r2 = r2_score(Y_test, Y_test_pred)\n",
    "            \n",
    "            # Store the result\n",
    "            results.append({\n",
    "                'Selected_features': selected_features,\n",
    "                'R-squared (R2)': r2\n",
    "            })\n",
    "\n",
    "# Convert results to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Find the best result based on the R² score\n",
    "best_result = results_df.sort_values(by='R-squared (R2)', ascending=False).iloc[0]\n",
    "\n",
    "# Extract the best features used\n",
    "best_features = best_result['Selected_features']\n",
    "\n",
    "# Create a new DataFrame with only the best features\n",
    "best_X = X_knn[best_features]\n",
    "best_Y = Y  # Assuming the target variable remains the same\n",
    "\n",
    "# Print the best R² score and the corresponding features\n",
    "print(f\"Best R² score: {best_result['R-squared (R2)']}\")\n",
    "print(\"Features used:\")\n",
    "print(best_features)\n",
    "\n",
    "# Display the new DataFrame\n",
    "best_X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c6c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a98d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define preprocessing for numerical features (scaling only)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a pipeline with scaling and the CatBoost model\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('scaler', scaler),\n",
    "    ('model', CatBoostRegressor(verbose=0))  # `verbose=0` to suppress CatBoost's training output\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets using the best features\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(best_X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model_pipeline.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "Y_train_pred = model_pipeline.predict(X_train)\n",
    "Y_test_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance using R² score\n",
    "best_r2 = r2_score(Y_test, Y_test_pred)\n",
    "print(f\"Best R² score on new model: {best_r2}\")\n",
    "#Best R² score on new model: 0.8374072940008164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49687ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the test split (20%)\n",
    "test_split = 0.2\n",
    "print(f\"\\nRunning for test split: {int(test_split * 100)}%\")\n",
    "\n",
    "# Split the data for the current test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(best_X, Y, test_size=test_split, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 600, 1100),\n",
    "        'depth': trial.suggest_int('depth', 8, 15),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.04),\n",
    "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-5, 1e-3),\n",
    "        'bagging_temperature': trial.suggest_uniform('bagging_temperature', 0.0, 2),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 400)\n",
    "    }\n",
    "   \n",
    "\n",
    "    # Train the model with the given parameters\n",
    "    cat_model = CatBoostRegressor(**params, random_state=42, verbose=0)\n",
    "    cat_model.fit(X_train, Y_train)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred = cat_model.predict(X_test)\n",
    "    \n",
    "    # Calculate the R2 score\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    \n",
    "    # Since Optuna minimizes the objective, we need to return the negative R2 score\n",
    "    return -r2\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Store the best parameters and the best R2 score\n",
    "best_params = study.best_params\n",
    "best_r2 = -study.best_value\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "final_model = CatBoostRegressor(**best_params, random_state=42, verbose=0)\n",
    "final_model.fit(X_train, Y_train)\n",
    "\n",
    "# Predict and evaluate the final model\n",
    "final_pred = final_model.predict(X_test)\n",
    "final_mse = mean_squared_error(Y_test, final_pred)\n",
    "final_r2 = r2_score(Y_test, final_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nResults for 20% Test Split:\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best R2 Score: {best_r2}\")\n",
    "print(f\"Final MSE: {final_mse}\")\n",
    "print(f\"Final R2 Score: {final_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd52eaf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d74e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve feature importance from the final model\n",
    "feature_importance = final_model.get_feature_importance()\n",
    "feature_names = best_X.columns  # Replace 'best_X' with your feature dataframe name if different\n",
    "\n",
    "# Create a DataFrame to hold feature names and their importance scores\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance for Final CatBoost Model')\n",
    "plt.gca().invert_yaxis()  # To display the highest importance feature at the top\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6609aafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict on the training set\n",
    "train_pred = final_model.predict(X_train)\n",
    "\n",
    "# Predict on the test set\n",
    "final_pred = final_model.predict(X_test)\n",
    "\n",
    "# Plot the actual vs predicted values for the training set\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(Y_train, train_pred, color='blue', edgecolors='k', alpha=0.6, label='Training Data')\n",
    "\n",
    "# Plot the actual vs predicted values for the test set\n",
    "plt.scatter(Y_test, final_pred, color='red', edgecolors='k', alpha=0.6, label='Test Data')\n",
    "\n",
    "# Plot the ideal line representing perfect prediction\n",
    "plt.plot([min(Y_train.min(), Y_test.min()), max(Y_train.max(), Y_test.max())], \n",
    "         [min(Y_train.min(), Y_test.min()), max(Y_train.max(), Y_test.max())], \n",
    "         color='red', linestyle='--', linewidth=2, label='Perfect Prediction Line')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted Values - CatBoost Model')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Add a grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Add annotation to indicate feature selection and fine-tuning\n",
    "plt.text(0.05, 0.95, 'Performance after feature selection and fine-tuning', \n",
    "         horizontalalignment='left', verticalalignment='top', \n",
    "         transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0, 0))\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f9a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_columns = ['Ionic conductivity', 'Relative density', 'source', 'MaterialFormula']\n",
    "df_output = df[output_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e58c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the relative differences between predictions and actual values\n",
    "relative_differences = np.abs(final_pred - Y_test.values) / np.abs(Y_test.values)\n",
    "\n",
    "# Get the indices of the 10 smallest relative differences\n",
    "best_indices = np.argsort(relative_differences)[:10]\n",
    "\n",
    "# Extract the best predictions and actual values for these indices\n",
    "best_preds = final_pred[best_indices]\n",
    "best_actuals = Y_test.values[best_indices]\n",
    "best_diff = relative_differences[best_indices]\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot predictions\n",
    "plt.bar(np.arange(10) - 0.2, best_preds, width=0.4, color='blue', alpha=0.7, label='Prediction')\n",
    "\n",
    "# Plot actual values\n",
    "plt.bar(np.arange(10) + 0.2, best_actuals, width=0.4, color='orange', alpha=0.7, label='Actual')\n",
    "\n",
    "plt.xticks(np.arange(10), best_indices, rotation=45)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Ionic Conductivity')\n",
    "plt.title('Top 10 Predictions vs Actual Values with Smallest Relative Differences')\n",
    "plt.legend()\n",
    "\n",
    "# Annotate predictions with percentage difference\n",
    "for i, (pred, diff) in enumerate(zip(best_preds, best_diff)):\n",
    "    plt.text(i - 0.2, pred + 0.02 * max(best_preds), f'{diff:.2%}', ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0986518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict on the training set\n",
    "train_pred = final_model.predict(X_train)\n",
    "\n",
    "# Plot the actual vs predicted values for the training set\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(Y_train, train_pred, color='blue', edgecolors='k', alpha=0.6, label='Training Data')\n",
    "\n",
    "# Plot the actual vs predicted values for the test set\n",
    "plt.scatter(Y_test, final_pred, color='red', edgecolors='k', alpha=0.6, label='Test Data')\n",
    "\n",
    "# Plot the ideal line representing perfect prediction\n",
    "plt.plot([min(Y_train.min(), Y_test.min()), max(Y_train.max(), Y_test.max())], \n",
    "         [min(Y_train.min(), Y_test.min()), max(Y_train.max(), Y_test.max())], \n",
    "         color='red', linestyle='--', linewidth=2, label='Perfect Prediction Line')\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0, 0))\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted Values - CatBoost Model')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868d5cc7",
   "metadata": {},
   "source": [
    "## Second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71577f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor,HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9fc3e2",
   "metadata": {},
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d9561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define preprocessing for numerical features (scaling only)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a pipeline with scaling and the CatBoost model\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('scaler', scaler),\n",
    "    ('model', CatBoostRegressor(verbose=0))  # `verbose=0` to suppress CatBoost's training output\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets using the best features\n",
    "X_train1, X_test1, Y_train1, Y_test1 = train_test_split(X_knn, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model_pipeline.fit(X_train1, Y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "Y_train_pred1 = model_pipeline.predict(X_train1)\n",
    "Y_test_pred1 = model_pipeline.predict(X_test1)\n",
    "\n",
    "# Evaluate the model's performance using R² score\n",
    "best_r2 = r2_score(Y_test1, Y_test_pred1)\n",
    "print(f\"Best R² score on new model: {best_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013b09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Scatter plot for the train set\n",
    "plt.scatter(Y_train1, Y_train_pred1, color='blue', alpha=0.5, label='Train Predictions (CatBoostRegressor)')\n",
    "# Scatter plot for the test set\n",
    "plt.scatter(Y_test1, Y_test_pred1, color='red', alpha=0.5, label='Test Predictions (CatBoostRegressor)')\n",
    "\n",
    "# Line for perfect predictions\n",
    "plt.plot([min(Y), max(Y)], [min(Y), max(Y)], 'k--', lw=2)\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Actual vs. Predicted Values for CatBoostRegressor')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "\n",
    "# Use scientific notation for the x-axis\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0, 0))\n",
    "\n",
    "# Removing grid\n",
    "plt.grid(False)\n",
    "\n",
    "# Legend\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a59829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define preprocessing for numerical features (scaling only)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a pipeline with scaling and the CatBoost model\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('scaler', scaler),\n",
    "    ('model', LGBMRegressor(verbose=0))  # `verbose=0` to suppress CatBoost's training output\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets using the best features\n",
    "X_train2, X_test2, Y_train2, Y_test2 = train_test_split(X_knn, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model_pipeline.fit(X_train2, Y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "Y_train_pred2 = model_pipeline.predict(X_train2)\n",
    "Y_test_pred2 = model_pipeline.predict(X_test2)\n",
    "\n",
    "# Evaluate the model's performance using R² score\n",
    "best_r2 = r2_score(Y_test2, Y_test_pred2)\n",
    "print(f\"Best R² score on new model: {best_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8384d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Scatter plot for the train set\n",
    "plt.scatter(Y_train2, Y_train_pred2, color='blue', alpha=0.5, label='Train Predictions (LGBMRegressor)')\n",
    "# Scatter plot for the test set\n",
    "plt.scatter(Y_test2, Y_test_pred2, color='red', alpha=0.5, label='Test Predictions (LGBMRegressor)')\n",
    "\n",
    "# Line for perfect predictions\n",
    "plt.plot([min(Y), max(Y)], [min(Y), max(Y)], 'k--', lw=2)\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Actual vs. Predicted Values for LGBMRegressor')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "\n",
    "# Use scientific notation for the x-axis\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0, 0))\n",
    "\n",
    "# Removing grid\n",
    "plt.grid(False)\n",
    "\n",
    "# Legend\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b5138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e429c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define preprocessing for numerical features (scaling only)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a pipeline with scaling and the CatBoost model\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('scaler', scaler),\n",
    "    ('model', LGBMRegressor(verbose=0))  # `verbose=0` to suppress CatBoost's training output\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets using the best features\n",
    "X_train1, X_test1, Y_train1, Y_test1 = train_test_split(X_knn, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model_pipeline.fit(X_train1, Y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "Y_train_pred = model_pipeline.predict(X_train1)\n",
    "Y_test_pred = model_pipeline.predict(X_test1)\n",
    "\n",
    "# Evaluate the model's performance using R² score\n",
    "best_r2 = r2_score(Y_test, Y_test_pred)\n",
    "print(f\"Best R² score on new model: {best_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c80066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
